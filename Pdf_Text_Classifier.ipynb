{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e38d314-97f5-4b1c-8c63-388d692861ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import PyPDF2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "465df508-2fcb-41e4-ac2c-bc5e4c08f7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vansh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vansh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Vansh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b57e60-482e-44ec-b5b5-bb70d1014c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            content = page.extract_text()\n",
    "            if content:\n",
    "                text += content\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc00d3fa-353d-43f1-a87b-1f7fd3ad1de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # remove extra whitespace\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(stemmer.stem(t)) for t in tokens]\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfec6ae-31f1-4e48-854a-35db6c5a1196",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = \"pdfs\"\n",
    "\n",
    "if not os.path.exists(pdf_dir):\n",
    "    raise FileNotFoundError(f\"Folder '{pdf_dir}' not found!\")\n",
    "\n",
    "label_map = {}\n",
    "for file in os.listdir(pdf_dir):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        if \"resume\" in file.lower():\n",
    "            label_map[file] = \"resume\"\n",
    "        elif \"invoice\" in file.lower():\n",
    "            label_map[file] = \"invoice\"\n",
    "        elif \"report\" in file.lower():\n",
    "            label_map[file] = \"report\"\n",
    "\n",
    "print(\"Total labeled PDFs:\", len(label_map))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef28957-eeb9-4ded-b20c-0311b6ddf41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for file, label in label_map.items():\n",
    "    path = os.path.join(pdf_dir, file)\n",
    "    raw_text = extract_text_from_pdf(path)\n",
    "    \n",
    "    if not raw_text.strip():\n",
    "        print(f\"Warning: '{file}' is empty or unreadable.\")\n",
    "        continue\n",
    "\n",
    "    clean_text = preprocess(raw_text)\n",
    "    texts.append(clean_text)\n",
    "    labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6731665e-82d8-42b9-90c2-d40305b0318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(labels).value_counts().plot(kind='bar', title='Class Distribution')\n",
    "plt.xlabel(\"Document Type\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcc4958-c74f-4b52-9ead-318341a69908",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=3000)\n",
    "X = tfidf.fit_transform(texts)\n",
    "\n",
    "y_labels = pd.Series(labels).astype('category')\n",
    "y = y_labels.cat.codes  # 0,1,2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdda58e2-54d9-46e9-8d0f-56b9e864d56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "# Convert y to NumPy array\n",
    "y_full = y.to_numpy()\n",
    "\n",
    "# Simulate unlabeled data (e.g., hide 20% labels)\n",
    "rng = np.random.default_rng(seed=42)\n",
    "unlabeled_indices = rng.choice(len(y_full), size=int(0.2 * len(y_full)), replace=False)\n",
    "y_semi = y_full.copy()\n",
    "y_semi[unlabeled_indices] = -1  # -1 indicates unlabeled\n",
    "\n",
    "# Train-test split only on labeled data\n",
    "labeled_mask = y_semi != -1\n",
    "X_labeled = X[labeled_mask]\n",
    "y_labeled = y_semi[labeled_mask]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_labeled, y_labeled, test_size=0.3, stratify=y_labeled, random_state=42\n",
    ")\n",
    "\n",
    "# Add back unlabeled data to training\n",
    "X_unlabeled = X[~labeled_mask]\n",
    "y_unlabeled = y_semi[~labeled_mask]\n",
    "\n",
    "X_train = vstack([X_train, X_unlabeled])\n",
    "y_train = np.concatenate([y_train, y_unlabeled])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840ff243-e792-4f5f-85f6-7a465e03d4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MultinomialNB()\n",
    "model = SelfTrainingClassifier(base_model)\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8791efd7-1a1b-4322-be0e-0a3e97d37666",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "present_classes = sorted(np.unique(np.concatenate((y_test, y_pred))))\n",
    "target_names = y_labels.cat.categories[present_classes]\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, labels=present_classes, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f5e99-258e-4da5-98d4-fa59b59b7641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(model, \"pdf_text_classifier_model.pkl\")\n",
    "joblib.dump(tfidf, \"tfidf_vectorizer.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77116d88-edc0-4ab3-91ef-e58e6e610bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99bd785-624f-4c58-8a70-abd3b0f91989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(model, \"pdf_text_classifier_model.pkl\")\n",
    "joblib.dump(tfidf, \"tfidf_vectorizer.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e910406-03ac-438c-b088-9dbc1fbe154f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
